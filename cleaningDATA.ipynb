{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6eed71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\dni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\dni\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\dni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dni\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.3 MB 991.0 kB/s eta 0:00:12\n",
      "   ---------------------------------------- 0.1/11.3 MB 991.0 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.2/11.3 MB 1.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/11.3 MB 2.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.5/11.3 MB 2.6 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/11.3 MB 4.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.7/11.3 MB 6.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.7/11.3 MB 5.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.7/11.3 MB 5.4 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.2/11.3 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.8/11.3 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.8/11.3 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.3/11.3 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.3/11.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.4/11.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.3 MB 9.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.7/11.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.3 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.8/11.3 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.3 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 14.2 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "   ---------------------------------------- 0.0/509.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 509.2/509.2 kB 10.6 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "   ---------------------------------------- 0.0/347.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 347.8/347.8 kB 10.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\dni\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705f68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "239116cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Original dataset has 684 rows\n",
      "Columns in the dataset: ['title', 'totalScore', 'reviewsCount', 'street', 'city', 'state', 'countryCode', 'website', 'phone', 'categoryName', 'url']\n",
      "\n",
      "Processing URLs...\n",
      "Checking all 684 rows in the 'website' column...\n",
      "Found 40 Facebook URLs\n",
      "Found 6 Instagram URLs\n",
      "\n",
      "Removing rows with null URLs...\n",
      "Removed 305 rows\n",
      "Final dataset has 379 rows\n",
      "\n",
      "Saving cleaned data to C:\\Users\\dni\\Desktop\\cleaned_dataset2.csv...\n",
      "Done!\n",
      "\n",
      "First 5 rows of cleaned data:\n",
      "                                    title  totalScore  reviewsCount  \\\n",
      "4        Doctor Sushi Nahariya סושי נהריה         4.8            30   \n",
      "5   אוסול - פלאפל, חומוס פול (סניף שלומי)         4.5           340   \n",
      "6                       HaTsuk Restaurant         4.1          1879   \n",
      "8                      Pizza Bar fornetto         4.5           758   \n",
      "11                              פלאפל איל         4.7           167   \n",
      "\n",
      "               street          city  state countryCode  \\\n",
      "4       Wolfson St 52     Nahariyya    NaN          IL   \n",
      "5     מסעף שלומי מערב        Shlomi    NaN          IL   \n",
      "6                 NaN  Rosh HaNikra    NaN          IL   \n",
      "8   תחנת דלק דור אלון  Gesher HaZiv    NaN          IL   \n",
      "11    Ha-Hagana St 20     Nahariyya    NaN          IL   \n",
      "\n",
      "                                              website             phone  \\\n",
      "4                              http://marea.pro/sushi  +972 52-367-0815   \n",
      "5   https://israelbusinessguide.com/catalog/shlomi...   +972 4-779-2909   \n",
      "6                               https://hatzuk.co.il/   +972 4-601-6849   \n",
      "8                              http://fornetto.co.il/   +972 4-900-1400   \n",
      "11  https://israelbusinessguide.com/catalog/naariy...               NaN   \n",
      "\n",
      "          categoryName                                                url  \n",
      "4     Sushi restaurant  https://www.google.com/maps/search/?api=1&quer...  \n",
      "5   Falafel restaurant  https://www.google.com/maps/search/?api=1&quer...  \n",
      "6    Kosher restaurant  https://www.google.com/maps/search/?api=1&quer...  \n",
      "8     Pizza restaurant  https://www.google.com/maps/search/?api=1&quer...  \n",
      "11          Restaurant  https://www.google.com/maps/search/?api=1&quer...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_csv_urls(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Clean CSV file by removing rows with Facebook or Instagram URLs\n",
    "    \n",
    "    Parameters:\n",
    "    input_file (str): Path to input CSV file\n",
    "    output_file (str): Path to output CSV file\n",
    "    \"\"\"\n",
    "\n",
    "    input_file = r\"C:\\Users\\dni\\Desktop\\dataset_crawler-google-places_2025-07-31_18-18-22-570.csv\"\n",
    "    output_file = r\"C:\\Users\\dni\\Desktop\\cleaned_dataset2.csv\"\n",
    "    \n",
    "\n",
    "    # Step 1: Read the CSV file\n",
    "    print(\"Reading CSV file...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Display initial info\n",
    "    print(f\"Original dataset has {len(df)} rows\")\n",
    "    print(f\"Columns in the dataset: {list(df.columns)}\")\n",
    "    \n",
    "    # Step 2: Check if 'website' column exists\n",
    "    if 'website' not in df.columns:\n",
    "        print(\"Error: 'website' column not found in the CSV file\")\n",
    "        print(f\"Available columns: {list(df.columns)}\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Replace Facebook and Instagram URLs with NaN (null)\n",
    "    print(\"\\nProcessing URLs...\")\n",
    "    print(f\"Checking all {len(df)} rows in the 'website' column...\")\n",
    "    \n",
    "    # Create a mask to identify Facebook and Instagram URLs\n",
    "    # This checks EVERY SINGLE ROW in the 'website' column\n",
    "    facebook_mask = df['website'].str.startswith('https://www.facebook', na=False)\n",
    "    instagram_mask = df['website'].str.startswith('https://www.instagram', na=False)\n",
    "    \n",
    "    # Count how many URLs will be removed\n",
    "    facebook_count = facebook_mask.sum()\n",
    "    instagram_count = instagram_mask.sum()\n",
    "    \n",
    "    print(f\"Found {facebook_count} Facebook URLs\")\n",
    "    print(f\"Found {instagram_count} Instagram URLs\")\n",
    "    \n",
    "    # Replace matching URLs with NaN\n",
    "    df.loc[facebook_mask | instagram_mask, 'website'] = np.nan\n",
    "    \n",
    "    # Step 4: Remove rows where 'website' is null/NaN\n",
    "    print(\"\\nRemoving rows with null URLs...\")\n",
    "    df_cleaned = df.dropna(subset=['website'])\n",
    "    \n",
    "    # Display results\n",
    "    removed_rows = len(df) - len(df_cleaned)\n",
    "    print(f\"Removed {removed_rows} rows\")\n",
    "    print(f\"Final dataset has {len(df_cleaned)} rows\")\n",
    "    \n",
    "    # Step 5: Save the cleaned data\n",
    "    print(f\"\\nSaving cleaned data to {output_file}...\")\n",
    "    df_cleaned.to_csv(output_file, index=False)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with your actual file paths\n",
    "    input_file = \"your_input_file.csv\"\n",
    "    output_file = \"cleaned_file.csv\"\n",
    "    \n",
    "    # Run the cleaning function\n",
    "    cleaned_data = clean_csv_urls(input_file, output_file)\n",
    "    \n",
    "    # Optional: Display first few rows of cleaned data\n",
    "    if cleaned_data is not None:\n",
    "        print(\"\\nFirst 5 rows of cleaned data:\")\n",
    "        print(cleaned_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
